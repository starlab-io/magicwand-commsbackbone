#ifndef common_config_h
#define common_config_h

/***************************************************************************
 * Defines configurable values that are used by the entire MW backbone
 * comms system. Do not include this file directly; rather it is to be
 * included indirectly via message_types.h or xen_keystore_defs.h.
 *
 * WARNING                                                 WARNING
 *
 *
 * To preserve changes, edit common_config.h.template. util/mw_prep
 * copies it to the right place.
 *
 * If you change this file, update (if needed) common_config.h and
 * rebuild everything.
 *
 * WARNING                                                 WARNING
 **************************************************************************/

#ifndef _common_config_defined
#  error "This file should not be included directly"
#endif

//
// Order of the block of shared memory for the Xen ring buffer. For
// instance, if the order is 6, we share 2^6 = 64 (0x40) pages, or
// 0x40000 bytes. The max we've tried is 6. Xen has an upper limit on
// how many pages can be shared. That limit can be configured by a Xen
// boot parameter - specifically look at gnttab_max_frames. N.B. we
// must create one grant reference per shared page.
//
#define XENEVENT_GRANT_REF_ORDER  6 // (2^order == page count)

//
// The items we're sharing on the ring buffer are big and we want to
// make the best use of the ring buffer. Optimize our sizes so we make
// use as much of the shared memory as possible. Note the tradeoff
// here: number of ring buffer slots vs size of each slot. Fewer slots
// will most certainly result in more EAGAIN errors, meaning that
// write attempts from the PVM will have to be retried. We make use of
// ring.h internals in the discussion below.
//
// The offset of the first element in shared memory is at
//     struct XXXX_sring.ring
//
// On on x86_64 this is at 
//     4 * sizeof(unsigned long) + 0n48 = 0n80 = 0x50
//
// So, provided we have at least 80 slots, we can re-appropriate 1
// byte from each slot to the shared ring header. Depending on
// application implementation, this could have the drawback of
// generating lots of small messages. Preliminary testing shows that a
// smaller message size (0x100) gives comparable times as larger
// sizes, likely due to the lower number of retries required due to a
// full ring.
//
// Useful sizes to consider:
//     mt_request_base_t: 0x18
//     non-buffer portion of mt_response_socket_recvfrom_t: 0x3a
//     non-buffer portion of mt_request_socket_send_t:      0x1c
//
//
// Ensure that MESSAGE_TARGET_MAX_SIZE is a power of 2 to make the
// best use of shared memory.
//

#define MESSAGE_TARGET_MAX_SIZE 0x100 // 256
//#define MESSAGE_TARGET_MAX_SIZE 0x200 // 512
//#define MESSAGE_TARGET_MAX_SIZE 0x400 // 1024
//#define MESSAGE_TARGET_MAX_SIZE 0x800 // 2048

#define MESSAGE_TYPE_MAX_PAYLOAD_LEN (MESSAGE_TARGET_MAX_SIZE - 0x3a - 1)

//
// The macros below determine whether we use the event channel. In the
// past we have observed scheduling problems on Rump that might make
// us want to disable the event channel. If it is disabled on side X,
// then: (1) X will poll on the ring buffer rather than wait on an
// event, and (2) the other side will not send an event. For
// performance reasons, you really do want to use the event channel.
//
// Here are approximate times for transferring 3 million bytes with
// assorted settings (yes = uses event channel, no = does not use
// event channel). We're using batch sending with the simple http
// server, whose buffer size is 0x1000; MESSAGE_TARGET_MAX_SIZE is
// 0x100
//
// INS yes, PVM yes: 1300ms
// INS yes, PVM no:  9500ms
// INS no,  PVM yes: 7500ms
// INS no,  PVM no:  8800ms
//

//
// Does the INS use an event channel callback for notification of
// request arrival?
//
#define INS_USES_EVENT_CHANNEL 1

//
// Does the PVM use an event channel callback for notification of
// response arrival?
//
#define PVM_USES_EVENT_CHANNEL 1


#endif //common_config_h
